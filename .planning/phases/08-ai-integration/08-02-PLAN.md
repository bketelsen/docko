---
phase: 08-ai-integration
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - internal/ai/ai.go
  - internal/ai/openai.go
  - internal/ai/anthropic.go
  - internal/ai/ollama.go
  - internal/ai/prompt.go
autonomous: true

must_haves:
  truths:
    - "Provider interface abstracts differences between OpenAI, Anthropic, and Ollama"
    - "Each provider returns structured suggestions with confidence scores"
    - "Providers correctly report availability based on env vars"
  artifacts:
    - path: "internal/ai/ai.go"
      provides: "Provider interface and types"
      exports: ["Provider", "AnalyzeRequest", "AnalyzeResponse", "Suggestion"]
    - path: "internal/ai/openai.go"
      provides: "OpenAI provider implementation"
      exports: ["OpenAIProvider"]
    - path: "internal/ai/anthropic.go"
      provides: "Anthropic provider implementation"
      exports: ["AnthropicProvider"]
    - path: "internal/ai/ollama.go"
      provides: "Ollama provider implementation"
      exports: ["OllamaProvider"]
    - path: "internal/ai/prompt.go"
      provides: "Prompt template building"
      exports: ["BuildPrompt"]
  key_links:
    - from: "internal/ai/openai.go"
      to: "OPENAI_API_KEY"
      via: "os.Getenv"
      pattern: "OPENAI_API_KEY"
    - from: "internal/ai/anthropic.go"
      to: "ANTHROPIC_API_KEY"
      via: "os.Getenv"
      pattern: "ANTHROPIC_API_KEY"
---

<objective>
Create AI provider interface and implementations for OpenAI, Anthropic, and Ollama.

Purpose: Abstract provider differences behind a common interface, enabling provider switching and fallback. Each provider uses its official Go SDK with structured JSON output for reliable tag/correspondent suggestions.
Output: Provider interface and three implementations ready for use by AI service.
</objective>

<execution_context>
@/home/bjk/.claude/get-shit-done/workflows/execute-plan.md
@/home/bjk/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/08-ai-integration/08-CONTEXT.md
@.planning/phases/08-ai-integration/08-RESEARCH.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create provider interface and types</name>
  <files>internal/ai/ai.go</files>
  <action>
Create the ai package with provider interface and shared types:

```go
package ai

import (
    "context"

    "github.com/google/uuid"
)

// Provider abstracts AI provider differences
type Provider interface {
    // Analyze sends document text and returns AI suggestions
    Analyze(ctx context.Context, req AnalyzeRequest) (*AnalyzeResponse, error)
    // Name returns the provider identifier (openai, anthropic, ollama)
    Name() string
    // Available checks if the provider is configured (env vars present)
    Available() bool
}

// AnalyzeRequest contains document content and context for AI analysis
type AnalyzeRequest struct {
    DocumentID     uuid.UUID
    TextContent    string   // First N pages of extracted text
    ExistingTags   []string // User's current tag taxonomy
    Correspondents []string // User's current correspondents
    MaxTokens      int      // Max response tokens (default 1024)
}

// AnalyzeResponse contains AI suggestions and usage data
type AnalyzeResponse struct {
    Suggestions []Suggestion
    Usage       Usage
}

// Suggestion represents a single AI recommendation
type Suggestion struct {
    Type       string  // "tag" or "correspondent"
    Value      string  // The suggested name
    Confidence float64 // 0.0 to 1.0
    Reasoning  string  // Brief explanation
    IsNew      bool    // True if not in existing taxonomy
}

// Usage tracks token consumption for cost monitoring
type Usage struct {
    InputTokens  int
    OutputTokens int
    Model        string
}

// AIResponse is the JSON schema for AI responses (all providers)
type AIResponse struct {
    Tags          []TagSuggestion          `json:"tags"`
    Correspondent *CorrespondentSuggestion `json:"correspondent"`
}

// TagSuggestion is the JSON schema for a tag suggestion
type TagSuggestion struct {
    Name       string  `json:"name"`
    Confidence float64 `json:"confidence"`
    Reasoning  string  `json:"reasoning"`
}

// CorrespondentSuggestion is the JSON schema for a correspondent suggestion
type CorrespondentSuggestion struct {
    Name       string  `json:"name"`
    Confidence float64 `json:"confidence"`
    Reasoning  string  `json:"reasoning"`
}

// ConvertToSuggestions converts AIResponse to []Suggestion
func ConvertToSuggestions(resp AIResponse, existingTags, existingCorrespondents []string) []Suggestion {
    var suggestions []Suggestion

    // Convert tag suggestions
    tagSet := make(map[string]bool)
    for _, t := range existingTags {
        tagSet[t] = true
    }
    for _, t := range resp.Tags {
        suggestions = append(suggestions, Suggestion{
            Type:       "tag",
            Value:      t.Name,
            Confidence: t.Confidence,
            Reasoning:  t.Reasoning,
            IsNew:      !tagSet[t.Name],
        })
    }

    // Convert correspondent suggestion
    if resp.Correspondent != nil {
        corrSet := make(map[string]bool)
        for _, c := range existingCorrespondents {
            corrSet[c] = true
        }
        suggestions = append(suggestions, Suggestion{
            Type:       "correspondent",
            Value:      resp.Correspondent.Name,
            Confidence: resp.Correspondent.Confidence,
            Reasoning:  resp.Correspondent.Reasoning,
            IsNew:      !corrSet[resp.Correspondent.Name],
        })
    }

    return suggestions
}
```
  </action>
  <verify>Check ./tmp/air-combined.log for compilation errors</verify>
  <done>ai package with Provider interface and types compiles</done>
</task>

<task type="auto">
  <name>Task 2: Create prompt template builder</name>
  <files>internal/ai/prompt.go</files>
  <action>
Create prompt building logic:

```go
package ai

import (
    "strings"
)

// SystemPrompt is the system instruction for AI analysis
const SystemPrompt = `You are a document analysis assistant. Analyze the provided document text and suggest:
1. Tags that categorize this document (e.g., invoice, receipt, contract, medical, insurance, tax, bank)
2. The correspondent (sender/recipient organization or person)

IMPORTANT:
- Prefer existing tags/correspondents when they match
- Only suggest new ones if no existing option fits well
- Assign confidence scores (0.0-1.0) based on how certain you are
- Provide brief reasoning for each suggestion
- Suggest 1-5 tags maximum, focusing on the most relevant
- Suggest exactly one correspondent (or omit if unclear)

Your response must be valid JSON matching the schema.`

// BuildPrompt constructs the user prompt with document context
func BuildPrompt(req AnalyzeRequest) string {
    var b strings.Builder

    b.WriteString("## Existing Tags\n")
    if len(req.ExistingTags) > 0 {
        for _, t := range req.ExistingTags {
            b.WriteString("- ")
            b.WriteString(t)
            b.WriteString("\n")
        }
    } else {
        b.WriteString("(none yet)\n")
    }

    b.WriteString("\n## Existing Correspondents\n")
    if len(req.Correspondents) > 0 {
        for _, c := range req.Correspondents {
            b.WriteString("- ")
            b.WriteString(c)
            b.WriteString("\n")
        }
    } else {
        b.WriteString("(none yet)\n")
    }

    b.WriteString("\n## Document Text (first pages)\n")
    b.WriteString(req.TextContent)

    return b.String()
}

// JSONSchema is the schema for structured output (OpenAI/Anthropic format)
var JSONSchema = map[string]any{
    "type": "object",
    "properties": map[string]any{
        "tags": map[string]any{
            "type": "array",
            "items": map[string]any{
                "type": "object",
                "properties": map[string]any{
                    "name":       map[string]any{"type": "string"},
                    "confidence": map[string]any{"type": "number"},
                    "reasoning":  map[string]any{"type": "string"},
                },
                "required": []string{"name", "confidence", "reasoning"},
            },
        },
        "correspondent": map[string]any{
            "type": "object",
            "properties": map[string]any{
                "name":       map[string]any{"type": "string"},
                "confidence": map[string]any{"type": "number"},
                "reasoning":  map[string]any{"type": "string"},
            },
            "required": []string{"name", "confidence", "reasoning"},
        },
    },
    "required": []string{"tags"},
}
```
  </action>
  <verify>Check ./tmp/air-combined.log for compilation errors</verify>
  <done>Prompt builder compiles with SystemPrompt and BuildPrompt</done>
</task>

<task type="auto">
  <name>Task 3: Create OpenAI provider implementation</name>
  <files>internal/ai/openai.go</files>
  <action>
First install the SDK: `go get github.com/openai/openai-go/v3`

Create OpenAI provider:

```go
package ai

import (
    "context"
    "encoding/json"
    "fmt"
    "os"

    "github.com/openai/openai-go/v3"
    "github.com/openai/openai-go/v3/option"
)

// OpenAIProvider implements Provider using the OpenAI API
type OpenAIProvider struct {
    client *openai.Client
}

// NewOpenAIProvider creates a new OpenAI provider
func NewOpenAIProvider() *OpenAIProvider {
    apiKey := os.Getenv("OPENAI_API_KEY")
    if apiKey == "" {
        return &OpenAIProvider{} // Not configured
    }
    return &OpenAIProvider{
        client: openai.NewClient(option.WithAPIKey(apiKey)),
    }
}

func (p *OpenAIProvider) Name() string {
    return "openai"
}

func (p *OpenAIProvider) Available() bool {
    return p.client != nil
}

func (p *OpenAIProvider) Analyze(ctx context.Context, req AnalyzeRequest) (*AnalyzeResponse, error) {
    if !p.Available() {
        return nil, fmt.Errorf("openai provider not configured")
    }

    maxTokens := req.MaxTokens
    if maxTokens == 0 {
        maxTokens = 1024
    }

    prompt := BuildPrompt(req)

    chatResp, err := p.client.Chat.Completions.New(ctx, openai.ChatCompletionNewParams{
        Model: openai.ChatModelGPT4oMini,
        Messages: []openai.ChatCompletionMessageParamUnion{
            openai.SystemMessage(SystemPrompt),
            openai.UserMessage(prompt),
        },
        ResponseFormat: openai.F[openai.ChatCompletionNewParamsResponseFormatUnion](
            openai.ResponseFormatJSONSchemaParam{
                Type: openai.F(openai.ResponseFormatJSONSchemaTypeJSONSchema),
                JSONSchema: openai.F(openai.ResponseFormatJSONSchemaJSONSchemaParam{
                    Name:   openai.F("document_analysis"),
                    Schema: openai.F(JSONSchema),
                    Strict: openai.Bool(true),
                }),
            },
        ),
        MaxTokens: openai.Int(int64(maxTokens)),
    })
    if err != nil {
        return nil, fmt.Errorf("openai completion: %w", err)
    }

    if len(chatResp.Choices) == 0 {
        return nil, fmt.Errorf("openai returned no choices")
    }

    var aiResp AIResponse
    if err := json.Unmarshal([]byte(chatResp.Choices[0].Message.Content), &aiResp); err != nil {
        return nil, fmt.Errorf("parse openai response: %w", err)
    }

    return &AnalyzeResponse{
        Suggestions: ConvertToSuggestions(aiResp, req.ExistingTags, req.Correspondents),
        Usage: Usage{
            InputTokens:  int(chatResp.Usage.PromptTokens),
            OutputTokens: int(chatResp.Usage.CompletionTokens),
            Model:        string(chatResp.Model),
        },
    }, nil
}
```
  </action>
  <verify>Check ./tmp/air-combined.log for compilation errors after go get</verify>
  <done>OpenAIProvider compiles and implements Provider interface</done>
</task>

<task type="auto">
  <name>Task 4: Create Anthropic provider implementation</name>
  <files>internal/ai/anthropic.go</files>
  <action>
First install the SDK: `go get github.com/anthropics/anthropic-sdk-go`

Create Anthropic provider:

```go
package ai

import (
    "context"
    "encoding/json"
    "fmt"
    "os"

    "github.com/anthropics/anthropic-sdk-go"
    "github.com/anthropics/anthropic-sdk-go/option"
)

// AnthropicProvider implements Provider using the Anthropic Claude API
type AnthropicProvider struct {
    client *anthropic.Client
}

// NewAnthropicProvider creates a new Anthropic provider
func NewAnthropicProvider() *AnthropicProvider {
    apiKey := os.Getenv("ANTHROPIC_API_KEY")
    if apiKey == "" {
        return &AnthropicProvider{} // Not configured
    }
    return &AnthropicProvider{
        client: anthropic.NewClient(option.WithAPIKey(apiKey)),
    }
}

func (p *AnthropicProvider) Name() string {
    return "anthropic"
}

func (p *AnthropicProvider) Available() bool {
    return p.client != nil
}

func (p *AnthropicProvider) Analyze(ctx context.Context, req AnalyzeRequest) (*AnalyzeResponse, error) {
    if !p.Available() {
        return nil, fmt.Errorf("anthropic provider not configured")
    }

    maxTokens := req.MaxTokens
    if maxTokens == 0 {
        maxTokens = 1024
    }

    prompt := BuildPrompt(req)

    message, err := p.client.Messages.New(ctx, anthropic.MessageNewParams{
        Model:     anthropic.ModelClaudeHaiku4_5,
        MaxTokens: int64(maxTokens),
        Messages: []anthropic.MessageParam{
            anthropic.NewUserMessage(anthropic.NewTextBlock(prompt)),
        },
        System: anthropic.F([]anthropic.TextBlockParam{
            anthropic.NewTextBlock(SystemPrompt),
        }),
    })
    if err != nil {
        return nil, fmt.Errorf("anthropic message: %w", err)
    }

    // Extract text content from response
    var content string
    for _, block := range message.Content {
        if block.Type == anthropic.ContentBlockTypeText {
            content = block.Text
            break
        }
    }

    if content == "" {
        return nil, fmt.Errorf("anthropic returned no text content")
    }

    var aiResp AIResponse
    if err := json.Unmarshal([]byte(content), &aiResp); err != nil {
        return nil, fmt.Errorf("parse anthropic response: %w", err)
    }

    return &AnalyzeResponse{
        Suggestions: ConvertToSuggestions(aiResp, req.ExistingTags, req.Correspondents),
        Usage: Usage{
            InputTokens:  int(message.Usage.InputTokens),
            OutputTokens: int(message.Usage.OutputTokens),
            Model:        string(message.Model),
        },
    }, nil
}
```

Note: Anthropic's haiku model is cost-effective for tagging tasks. The SDK may need adjustment based on actual API structure - follow SDK documentation if compilation errors occur.
  </action>
  <verify>Check ./tmp/air-combined.log for compilation errors after go get</verify>
  <done>AnthropicProvider compiles and implements Provider interface</done>
</task>

<task type="auto">
  <name>Task 5: Create Ollama provider implementation</name>
  <files>internal/ai/ollama.go</files>
  <action>
First install the SDK: `go get github.com/ollama/ollama/api`

Create Ollama provider:

```go
package ai

import (
    "context"
    "encoding/json"
    "fmt"
    "os"
    "strings"

    "github.com/ollama/ollama/api"
)

// OllamaProvider implements Provider using a local Ollama instance
type OllamaProvider struct {
    model string
}

// NewOllamaProvider creates a new Ollama provider
func NewOllamaProvider() *OllamaProvider {
    model := os.Getenv("OLLAMA_MODEL")
    if model == "" {
        model = "llama3.2" // Default model
    }
    return &OllamaProvider{model: model}
}

func (p *OllamaProvider) Name() string {
    return "ollama"
}

func (p *OllamaProvider) Available() bool {
    // Ollama is available if OLLAMA_HOST is set or we can connect to default
    // For simplicity, we always return true and handle connection errors at runtime
    return os.Getenv("OLLAMA_HOST") != "" || true // Allow default localhost
}

func (p *OllamaProvider) Analyze(ctx context.Context, req AnalyzeRequest) (*AnalyzeResponse, error) {
    client, err := api.ClientFromEnvironment()
    if err != nil {
        return nil, fmt.Errorf("create ollama client: %w", err)
    }

    prompt := BuildPrompt(req)

    // Build full prompt with system instruction for Ollama
    fullPrompt := fmt.Sprintf("%s\n\n%s\n\nRespond with valid JSON only.", SystemPrompt, prompt)

    var response strings.Builder
    var evalCount, promptEvalCount int

    stream := false
    err = client.Generate(ctx, &api.GenerateRequest{
        Model:  p.model,
        Prompt: fullPrompt,
        Format: json.RawMessage(`"json"`),
        Stream: &stream,
        Options: map[string]any{
            "temperature": 0.1, // Low temperature for consistent output
        },
    }, func(resp api.GenerateResponse) error {
        response.WriteString(resp.Response)
        if resp.Done {
            evalCount = resp.EvalCount
            promptEvalCount = resp.PromptEvalCount
        }
        return nil
    })
    if err != nil {
        return nil, fmt.Errorf("ollama generate: %w", err)
    }

    var aiResp AIResponse
    if err := json.Unmarshal([]byte(response.String()), &aiResp); err != nil {
        return nil, fmt.Errorf("parse ollama response: %w (raw: %s)", err, response.String())
    }

    return &AnalyzeResponse{
        Suggestions: ConvertToSuggestions(aiResp, req.ExistingTags, req.Correspondents),
        Usage: Usage{
            InputTokens:  promptEvalCount,
            OutputTokens: evalCount,
            Model:        p.model,
        },
    }, nil
}
```

Note: Ollama uses Generate API for simpler JSON output. The api package structure may vary - adjust based on actual SDK.
  </action>
  <verify>Check ./tmp/air-combined.log for compilation errors after go get</verify>
  <done>OllamaProvider compiles and implements Provider interface</done>
</task>

</tasks>

<verification>
1. All providers compile: No errors in ./tmp/air-combined.log
2. SDKs installed: `go list -m github.com/openai/openai-go/v3 github.com/anthropics/anthropic-sdk-go github.com/ollama/ollama/api`
3. Interface satisfaction: Each provider implements Provider interface
4. Types exported: AnalyzeRequest, AnalyzeResponse, Suggestion available from ai package
</verification>

<success_criteria>
- Provider interface abstracts OpenAI, Anthropic, and Ollama
- Each provider uses official Go SDK
- Structured JSON output with confidence scores
- Prompt includes existing tags/correspondents for context
- All code compiles without errors
</success_criteria>

<output>
After completion, create `.planning/phases/08-ai-integration/08-02-SUMMARY.md`
</output>
