---
phase: 01-foundation
plan: 03
type: execute
wave: 3
depends_on: ["01-01", "01-02"]
files_modified:
  - internal/document/document.go
  - internal/config/config.go
autonomous: true

must_haves:
  truths:
    - "Document service can store a file and create database record"
    - "Document events are logged for each processing step"
    - "STORAGE_PATH environment variable configures file storage location"
    - "Duplicate documents are detected by content hash before storage"
  artifacts:
    - path: "internal/document/document.go"
      provides: "Document service coordinating storage, database, queue"
      exports: ["Service", "New", "Ingest", "GetByID", "LogEvent"]
      min_lines: 100
    - path: "internal/config/config.go"
      provides: "Configuration with STORAGE_PATH"
      contains: "StoragePath"
  key_links:
    - from: "internal/document/document.go"
      to: "internal/storage/storage.go"
      via: "storage operations"
      pattern: "storage\\.CopyAndHash"
    - from: "internal/document/document.go"
      to: "internal/database/sqlc/documents.sql.go"
      via: "database queries"
      pattern: "Queries\\.CreateDocument"
    - from: "internal/document/document.go"
      to: "internal/queue/queue.go"
      via: "job enqueuing"
      pattern: "queue\\.EnqueueTx"
---

<objective>
Create the document service that coordinates storage, database, and queue operations.

Purpose: The document service is the main entry point for document operations. It orchestrates file storage (with UUID path generation), database persistence (document metadata + events), and job queue integration (for async processing). This connects all the foundation pieces.

Output: internal/document/document.go service, updated config.go with STORAGE_PATH.
</objective>

<execution_context>
@/home/bjk/.claude/get-shit-done/workflows/execute-plan.md
@/home/bjk/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/phases/01-foundation/01-CONTEXT.md
@.planning/phases/01-foundation/01-01-SUMMARY.md
@.planning/phases/01-foundation/01-02-SUMMARY.md
@internal/storage/storage.go
@internal/queue/queue.go
@internal/database/sqlc/documents.sql.go
@internal/config/config.go
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add STORAGE_PATH to config</name>
  <files>internal/config/config.go</files>
  <action>
Update internal/config/config.go to add storage configuration:

1. Add StorageConfig struct:
```go
type StorageConfig struct {
    Path string // Root path for document storage
}
```

2. Add Storage field to Config struct:
```go
type Config struct {
    DatabaseURL string
    Port        string
    Env         string
    Site        SiteConfig
    Auth        AuthConfig
    Storage     StorageConfig  // ADD THIS
}
```

3. In Load(), populate Storage:
```go
Storage: StorageConfig{
    Path: getEnvOrDefault("STORAGE_PATH", "./storage"),
},
```

4. After loading, validate storage path can be used (warn if not set):
```go
if cfg.Storage.Path == "./storage" {
    slog.Warn("STORAGE_PATH not set, using ./storage")
}
```

Keep all existing code, just add the new Storage configuration.
  </action>
  <verify>
Run `make dev` and check ./tmp/air-combined.log. Server should start. Log should show "STORAGE_PATH not set, using ./storage" warning.
  </verify>
  <done>Config loads STORAGE_PATH with default of ./storage. Warning logged if using default.</done>
</task>

<task type="auto">
  <name>Task 2: Create document service</name>
  <files>internal/document/document.go</files>
  <action>
Create internal/document/document.go:

```go
package document

import (
    "context"
    "encoding/json"
    "fmt"
    "log/slog"
    "path/filepath"
    "time"

    "github.com/google/uuid"
    "github.com/jackc/pgx/v5"

    "docko/internal/database"
    "docko/internal/database/sqlc"
    "docko/internal/queue"
    "docko/internal/storage"
)

// Event types for audit trail
const (
    EventIngested       = "ingested"
    EventHashed         = "hashed"
    EventDuplicateFound = "duplicate_found"
    EventTextExtracted  = "text_extracted"
    EventThumbnailGenerated = "thumbnail_generated"
    EventFailed         = "failed"
)

// Queue names
const (
    QueueDefault = "default"
)

// Job types
const (
    JobTypeProcess = "process_document"
)

// IngestPayload is the job payload for document processing
type IngestPayload struct {
    DocumentID uuid.UUID `json:"document_id"`
}

// Service handles document operations
type Service struct {
    db      *database.DB
    storage *storage.Storage
    queue   *queue.Queue
}

// New creates a new document Service
func New(db *database.DB, storage *storage.Storage, queue *queue.Queue) *Service {
    return &Service{
        db:      db,
        storage: storage,
        queue:   queue,
    }
}

// Ingest stores a new document from a source file path
// Returns the document ID, or existing document ID if duplicate
func (s *Service) Ingest(ctx context.Context, sourcePath, originalFilename string) (*sqlc.Document, bool, error) {
    start := time.Now()
    docID := uuid.New()

    // Compute destination path
    destPath := s.storage.PathForUUID(storage.CategoryOriginals, docID, filepath.Ext(originalFilename))

    // Copy file and compute hash in single pass
    contentHash, fileSize, err := s.storage.CopyAndHash(destPath, sourcePath)
    if err != nil {
        return nil, false, fmt.Errorf("copy and hash file: %w", err)
    }

    // Check for duplicate
    existing, err := s.db.Queries.GetDocumentByHash(ctx, contentHash)
    if err == nil {
        // Duplicate found - clean up copied file and return existing
        s.storage.Delete(destPath)
        slog.Info("duplicate document detected", "existing_id", existing.ID, "hash", contentHash[:16]+"...")

        // Log duplicate event on existing document
        s.LogEvent(ctx, existing.ID, EventDuplicateFound, map[string]any{
            "attempted_filename": originalFilename,
            "source_path":        sourcePath,
        }, nil, time.Since(start))

        return &existing, true, nil
    }
    if err != pgx.ErrNoRows {
        // Unexpected error
        s.storage.Delete(destPath)
        return nil, false, fmt.Errorf("check duplicate: %w", err)
    }

    // Start transaction for document + job creation
    tx, err := s.db.Pool.Begin(ctx)
    if err != nil {
        s.storage.Delete(destPath)
        return nil, false, fmt.Errorf("begin transaction: %w", err)
    }
    defer tx.Rollback(ctx)

    qtx := s.db.Queries.WithTx(tx)

    // Create document record
    doc, err := qtx.CreateDocument(ctx, sqlc.CreateDocumentParams{
        OriginalFilename: originalFilename,
        ContentHash:      contentHash,
        FileSize:         fileSize,
        // page_count, pdf_title, pdf_author, pdf_created_at filled by processing job
    })
    if err != nil {
        s.storage.Delete(destPath)
        return nil, false, fmt.Errorf("create document: %w", err)
    }

    // Log ingested event
    eventPayload, _ := json.Marshal(map[string]any{
        "source_path": sourcePath,
        "dest_path":   destPath,
        "file_size":   fileSize,
        "hash":        contentHash,
    })
    _, err = qtx.CreateDocumentEvent(ctx, sqlc.CreateDocumentEventParams{
        DocumentID:   doc.ID,
        EventType:    EventIngested,
        Payload:      eventPayload,
        DurationMs:   intPtr(int32(time.Since(start).Milliseconds())),
    })
    if err != nil {
        s.storage.Delete(destPath)
        return nil, false, fmt.Errorf("create event: %w", err)
    }

    // Enqueue processing job
    _, err = s.queue.EnqueueTx(ctx, qtx, QueueDefault, JobTypeProcess, IngestPayload{
        DocumentID: doc.ID,
    })
    if err != nil {
        s.storage.Delete(destPath)
        return nil, false, fmt.Errorf("enqueue job: %w", err)
    }

    if err := tx.Commit(ctx); err != nil {
        s.storage.Delete(destPath)
        return nil, false, fmt.Errorf("commit transaction: %w", err)
    }

    slog.Info("document ingested", "id", doc.ID, "filename", originalFilename, "size", fileSize, "hash", contentHash[:16]+"...")

    return &doc, false, nil
}

// GetByID retrieves a document by ID
func (s *Service) GetByID(ctx context.Context, id uuid.UUID) (*sqlc.Document, error) {
    doc, err := s.db.Queries.GetDocument(ctx, id)
    if err != nil {
        return nil, fmt.Errorf("get document: %w", err)
    }
    return &doc, nil
}

// GetByHash retrieves a document by content hash
func (s *Service) GetByHash(ctx context.Context, hash string) (*sqlc.Document, error) {
    doc, err := s.db.Queries.GetDocumentByHash(ctx, hash)
    if err != nil {
        return nil, fmt.Errorf("get document by hash: %w", err)
    }
    return &doc, nil
}

// GetEvents retrieves all events for a document
func (s *Service) GetEvents(ctx context.Context, id uuid.UUID) ([]sqlc.DocumentEvent, error) {
    events, err := s.db.Queries.GetDocumentEvents(ctx, id)
    if err != nil {
        return nil, fmt.Errorf("get events: %w", err)
    }
    return events, nil
}

// LogEvent creates an audit trail event for a document
func (s *Service) LogEvent(ctx context.Context, docID uuid.UUID, eventType string, payload map[string]any, errMsg *string, duration time.Duration) error {
    var payloadJSON []byte
    if payload != nil {
        payloadJSON, _ = json.Marshal(payload)
    }

    _, err := s.db.Queries.CreateDocumentEvent(ctx, sqlc.CreateDocumentEventParams{
        DocumentID:   docID,
        EventType:    eventType,
        Payload:      payloadJSON,
        ErrorMessage: errMsg,
        DurationMs:   intPtr(int32(duration.Milliseconds())),
    })
    if err != nil {
        return fmt.Errorf("create event: %w", err)
    }
    return nil
}

// OriginalPath returns the file path for a document's original file
func (s *Service) OriginalPath(doc *sqlc.Document) string {
    return s.storage.PathForUUID(storage.CategoryOriginals, doc.ID, filepath.Ext(doc.OriginalFilename))
}

// ThumbnailPath returns the file path for a document's thumbnail
func (s *Service) ThumbnailPath(doc *sqlc.Document) string {
    return s.storage.PathForUUID(storage.CategoryThumbnails, doc.ID, ".png")
}

// TextPath returns the file path for a document's extracted text
func (s *Service) TextPath(doc *sqlc.Document) string {
    return s.storage.PathForUUID(storage.CategoryText, doc.ID, ".txt")
}

func intPtr(i int32) *int32 {
    return &i
}
```

Key implementation details:
- Ingest() creates document atomically: copy file + hash -> check duplicate -> transaction (create doc + event + job) -> commit
- On any failure, cleans up copied file before returning error
- Duplicate detection by content hash - returns existing doc instead of creating new
- Events logged with timing information
- All operations use wrapped errors for context
- Paths for derived files (thumbnails, text) ready for future phases
  </action>
  <verify>
Run `make dev` and check ./tmp/air-combined.log. The document package should compile without errors.
  </verify>
  <done>internal/document/document.go exists and compiles. Exports: Service, New, Ingest, GetByID, GetByHash, GetEvents, LogEvent, OriginalPath, ThumbnailPath, TextPath.</done>
</task>

<task type="auto">
  <name>Task 3: Wire services in main.go</name>
  <files>cmd/server/main.go</files>
  <action>
Update cmd/server/main.go to initialize storage, queue, and document services.

1. Add imports for new packages:
```go
import (
    // existing imports...
    "docko/internal/document"
    "docko/internal/queue"
    "docko/internal/storage"
)
```

2. After database initialization, add:
```go
// Initialize storage
store, err := storage.New(cfg.Storage.Path)
if err != nil {
    slog.Error("failed to initialize storage", "error", err)
    os.Exit(1)
}
slog.Info("storage initialized", "path", cfg.Storage.Path)

// Initialize queue
q := queue.New(db, queue.DefaultConfig())

// Initialize document service
docService := document.New(db, store, q)
_ = docService // Will be used by handlers in future phases
```

3. Do NOT start the queue workers yet (no handlers registered). Just initialize.
The queue will be started when we have processing handlers (future phases).

4. Add graceful shutdown for queue:
```go
// In shutdown section, before db.Close():
q.Stop()
```

Note: Only add the initialization. Do not register handlers or start workers in this phase - that comes when we have actual processing jobs (Phase 3).
  </action>
  <verify>
Run `make dev`. Check ./tmp/air-combined.log for:
1. No compilation errors
2. Log line: "storage initialized" with path
3. Server starts successfully on configured port
  </verify>
  <done>main.go initializes storage, queue, and document service. Storage path logged. Server starts successfully.</done>
</task>

</tasks>

<verification>
1. Config has STORAGE_PATH: `grep "StoragePath\|STORAGE_PATH" internal/config/config.go` shows both
2. Document service compiles: `make dev` shows no errors
3. Services wired in main: `grep "document.New\|storage.New\|queue.New" cmd/server/main.go` shows all three
4. Server starts: `make dev` logs "storage initialized" and server listens on port
</verification>

<success_criteria>
- Config loads STORAGE_PATH with sensible default (./storage)
- Document service coordinates storage + database + queue
- Ingest() is transactional: document + event + job in single transaction
- Duplicate detection by content hash returns existing document
- Events logged with timing and error details
- Services initialized in main.go (but queue not started yet)
- All code uses slog, fmt.Errorf with %w
</success_criteria>

<output>
After completion, create `.planning/phases/01-foundation/01-03-SUMMARY.md`
</output>
