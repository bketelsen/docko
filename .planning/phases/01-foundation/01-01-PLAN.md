---
phase: 01-foundation
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - internal/database/migrations/003_documents.sql
  - sqlc/queries/documents.sql
  - sqlc/queries/jobs.sql
  - internal/storage/storage.go
autonomous: true

must_haves:
  truths:
    - "Documents table exists with UUID primary key"
    - "Jobs table exists with SKIP LOCKED compatible schema"
    - "Document events table exists for audit trail"
    - "Storage service can compute paths from UUIDs"
    - "Storage service can hash files while copying"
  artifacts:
    - path: "internal/database/migrations/003_documents.sql"
      provides: "Documents, jobs, document_events, tags, correspondents tables"
      contains: "CREATE TABLE documents"
    - path: "sqlc/queries/documents.sql"
      provides: "Document CRUD queries"
      contains: "CreateDocument"
    - path: "sqlc/queries/jobs.sql"
      provides: "Job queue queries with SKIP LOCKED"
      contains: "FOR UPDATE SKIP LOCKED"
    - path: "internal/storage/storage.go"
      provides: "File storage operations"
      exports: ["New", "PathForUUID", "CopyAndHash", "EnsureDirectories"]
  key_links:
    - from: "sqlc/queries/jobs.sql"
      to: "internal/database/migrations/003_documents.sql"
      via: "jobs table schema"
      pattern: "FROM jobs"
    - from: "internal/storage/storage.go"
      to: "STORAGE_PATH"
      via: "environment variable"
      pattern: "basePath"
---

<objective>
Create the database schema for documents, jobs, and audit events, plus the file storage service.

Purpose: This plan establishes the data layer foundations. The database schema defines how documents, processing jobs, and audit events are stored. The storage service handles file system operations with UUID-based sharding.

Output: Database migration, sqlc queries for documents and jobs, storage service package.
</objective>

<execution_context>
@/home/bjk/.claude/get-shit-done/workflows/execute-plan.md
@/home/bjk/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-foundation/01-CONTEXT.md
@.planning/phases/01-foundation/01-RESEARCH.md
@internal/database/database.go
@internal/database/migrations/002_admin_users.sql
@sqlc/sqlc.yaml
@internal/config/config.go
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create database migration for documents, jobs, and events</name>
  <files>internal/database/migrations/003_documents.sql</files>
  <action>
Create goose migration with the following tables:

**documents table:**
- id UUID PRIMARY KEY DEFAULT gen_random_uuid()
- original_filename VARCHAR(255) NOT NULL
- content_hash VARCHAR(64) NOT NULL (SHA256 hex)
- file_size BIGINT NOT NULL
- page_count INT
- pdf_title TEXT (extracted from PDF metadata)
- pdf_author TEXT (extracted from PDF metadata)
- pdf_created_at TIMESTAMPTZ (extracted from PDF metadata)
- document_date TIMESTAMPTZ (user-settable, defaults to created_at)
- created_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
- updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
- Add UNIQUE constraint on content_hash for duplicate detection

**job_status enum:** 'pending', 'processing', 'completed', 'failed'

**jobs table:**
- id UUID PRIMARY KEY DEFAULT gen_random_uuid()
- queue_name VARCHAR(50) NOT NULL DEFAULT 'default'
- job_type VARCHAR(50) NOT NULL
- payload JSONB NOT NULL
- status job_status NOT NULL DEFAULT 'pending'
- attempt INT NOT NULL DEFAULT 0
- max_attempts INT NOT NULL DEFAULT 3
- scheduled_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
- visible_until TIMESTAMPTZ
- started_at TIMESTAMPTZ
- completed_at TIMESTAMPTZ
- last_error TEXT
- created_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
- updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
- Index on (queue_name, status, scheduled_at, created_at) WHERE status IN ('pending', 'processing')

**document_events table:**
- id UUID PRIMARY KEY DEFAULT gen_random_uuid()
- document_id UUID NOT NULL REFERENCES documents(id) ON DELETE CASCADE
- event_type VARCHAR(50) NOT NULL (e.g., 'ingested', 'hashed', 'text_extracted')
- payload JSONB (event-specific data)
- error_message TEXT
- duration_ms INT
- created_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
- Index on (document_id, created_at DESC)

**Junction tables (empty, for future phases):**
- tags: id UUID, name VARCHAR(100) UNIQUE, color VARCHAR(7), created_at
- correspondents: id UUID, name VARCHAR(255), created_at
- document_tags: document_id UUID, tag_id UUID, PRIMARY KEY (document_id, tag_id)
- document_correspondents: document_id UUID, correspondent_id UUID (one correspondent per doc)

Include +goose Down to drop all tables in reverse order.
  </action>
  <verify>
Run `make dev` and check ./tmp/air-combined.log for migration errors. Check that tables exist:
`docker exec -it docko-db-1 psql -U docko -c "\dt"` should list: documents, jobs, document_events, tags, correspondents, document_tags, document_correspondents
  </verify>
  <done>Migration 003 applies successfully. All 7 tables exist with correct columns and indexes.</done>
</task>

<task type="auto">
  <name>Task 2: Create sqlc queries for documents and jobs</name>
  <files>sqlc/queries/documents.sql, sqlc/queries/jobs.sql</files>
  <action>
**documents.sql queries:**

```sql
-- name: CreateDocument :one
INSERT INTO documents (original_filename, content_hash, file_size, page_count, pdf_title, pdf_author, pdf_created_at, document_date)
VALUES ($1, $2, $3, $4, $5, $6, $7, COALESCE($8, NOW()))
RETURNING *;

-- name: GetDocument :one
SELECT * FROM documents WHERE id = $1;

-- name: GetDocumentByHash :one
SELECT * FROM documents WHERE content_hash = $1;

-- name: ListDocuments :many
SELECT * FROM documents ORDER BY created_at DESC LIMIT $1 OFFSET $2;

-- name: UpdateDocument :one
UPDATE documents SET
  document_date = COALESCE($2, document_date),
  updated_at = NOW()
WHERE id = $1
RETURNING *;

-- name: DeleteDocument :exec
DELETE FROM documents WHERE id = $1;

-- name: CreateDocumentEvent :one
INSERT INTO document_events (document_id, event_type, payload, error_message, duration_ms)
VALUES ($1, $2, $3, $4, $5)
RETURNING *;

-- name: GetDocumentEvents :many
SELECT * FROM document_events WHERE document_id = $1 ORDER BY created_at DESC;

-- name: GetLatestDocumentEvent :one
SELECT * FROM document_events WHERE document_id = $1 ORDER BY created_at DESC LIMIT 1;
```

**jobs.sql queries:**

```sql
-- name: EnqueueJob :one
INSERT INTO jobs (queue_name, job_type, payload, max_attempts, scheduled_at)
VALUES ($1, $2, $3, COALESCE($4, 3), COALESCE($5, NOW()))
RETURNING *;

-- name: DequeueJobs :many
WITH next_jobs AS (
    SELECT id FROM jobs
    WHERE queue_name = $1
      AND (status = 'pending' OR (status = 'processing' AND visible_until < NOW()))
      AND scheduled_at <= NOW()
      AND attempt < max_attempts
    ORDER BY created_at
    LIMIT $2
    FOR UPDATE SKIP LOCKED
)
UPDATE jobs
SET status = 'processing',
    attempt = attempt + 1,
    started_at = NOW(),
    visible_until = NOW() + INTERVAL '5 minutes',
    updated_at = NOW()
FROM next_jobs
WHERE jobs.id = next_jobs.id
RETURNING jobs.*;

-- name: CompleteJob :one
UPDATE jobs
SET status = 'completed',
    completed_at = NOW(),
    updated_at = NOW()
WHERE id = $1
RETURNING *;

-- name: FailJob :one
UPDATE jobs
SET status = 'failed',
    last_error = $2,
    updated_at = NOW()
WHERE id = $1
RETURNING *;

-- name: RetryJob :one
UPDATE jobs
SET status = 'pending',
    scheduled_at = $2,
    updated_at = NOW()
WHERE id = $1
RETURNING *;

-- name: GetJob :one
SELECT * FROM jobs WHERE id = $1;

-- name: GetPendingJobCount :one
SELECT COUNT(*) FROM jobs WHERE queue_name = $1 AND status = 'pending';

-- name: GetFailedJobs :many
SELECT * FROM jobs WHERE queue_name = $1 AND status = 'failed' ORDER BY updated_at DESC LIMIT $2;
```

After creating both files, run `make generate` to regenerate sqlc code.
  </action>
  <verify>
Check ./tmp/air-combined.log for sqlc generation errors. Verify generated files exist:
`ls internal/database/sqlc/documents.sql.go internal/database/sqlc/jobs.sql.go`
  </verify>
  <done>sqlc generates without errors. documents.sql.go and jobs.sql.go exist with correct query functions.</done>
</task>

<task type="auto">
  <name>Task 3: Create storage service package</name>
  <files>internal/storage/storage.go</files>
  <action>
Create internal/storage/storage.go with:

```go
package storage

import (
    "crypto/sha256"
    "fmt"
    "io"
    "os"
    "path/filepath"

    "github.com/google/uuid"
)

// Storage handles file system operations for documents
type Storage struct {
    basePath string
}

// Categories for different file types
const (
    CategoryOriginals  = "originals"
    CategoryThumbnails = "thumbnails"
    CategoryText       = "text"
)

// New creates a Storage instance with the given base path
func New(basePath string) (*Storage, error) {
    if basePath == "" {
        return nil, fmt.Errorf("storage path cannot be empty")
    }

    s := &Storage{basePath: basePath}

    // Ensure base directories exist
    if err := s.EnsureDirectories(); err != nil {
        return nil, fmt.Errorf("failed to create directories: %w", err)
    }

    return s, nil
}

// EnsureDirectories creates the required directory structure
func (s *Storage) EnsureDirectories() error {
    categories := []string{CategoryOriginals, CategoryThumbnails, CategoryText}
    for _, cat := range categories {
        path := filepath.Join(s.basePath, cat)
        if err := os.MkdirAll(path, 0755); err != nil {
            return fmt.Errorf("failed to create %s directory: %w", cat, err)
        }
    }
    return nil
}

// PathForUUID returns the full file path for a UUID in a category
// Uses 2-level sharding: ab/c1/abc12345-...
func (s *Storage) PathForUUID(category string, id uuid.UUID, ext string) string {
    str := id.String()
    return filepath.Join(s.basePath, category, str[0:2], str[2:4], str+ext)
}

// DirForUUID returns the directory path for a UUID in a category
func (s *Storage) DirForUUID(category string, id uuid.UUID) string {
    str := id.String()
    return filepath.Join(s.basePath, category, str[0:2], str[2:4])
}

// CopyAndHash copies a file to destination while computing SHA256 hash
// Returns the hex-encoded hash and any error
func (s *Storage) CopyAndHash(dst, src string) (string, int64, error) {
    in, err := os.Open(src)
    if err != nil {
        return "", 0, fmt.Errorf("open source: %w", err)
    }
    defer in.Close()

    // Ensure destination directory exists
    if err := os.MkdirAll(filepath.Dir(dst), 0755); err != nil {
        return "", 0, fmt.Errorf("create directory: %w", err)
    }

    out, err := os.Create(dst)
    if err != nil {
        return "", 0, fmt.Errorf("create destination: %w", err)
    }
    defer out.Close()

    hash := sha256.New()
    tee := io.TeeReader(in, hash)

    size, err := io.Copy(out, tee)
    if err != nil {
        // Clean up partial file
        os.Remove(dst)
        return "", 0, fmt.Errorf("copy file: %w", err)
    }

    if err := out.Sync(); err != nil {
        return "", 0, fmt.Errorf("sync file: %w", err)
    }

    return fmt.Sprintf("%x", hash.Sum(nil)), size, nil
}

// HashFile computes SHA256 hash of a file without copying
func (s *Storage) HashFile(path string) (string, error) {
    f, err := os.Open(path)
    if err != nil {
        return "", fmt.Errorf("open file: %w", err)
    }
    defer f.Close()

    h := sha256.New()
    if _, err := io.Copy(h, f); err != nil {
        return "", fmt.Errorf("hash file: %w", err)
    }

    return fmt.Sprintf("%x", h.Sum(nil)), nil
}

// FileExists checks if a file exists at the given path
func (s *Storage) FileExists(path string) bool {
    _, err := os.Stat(path)
    return err == nil
}

// Delete removes a file at the given path
func (s *Storage) Delete(path string) error {
    return os.Remove(path)
}

// BasePath returns the storage base path
func (s *Storage) BasePath() string {
    return s.basePath
}
```

Follow existing codebase patterns:
- Use fmt.Errorf with %w for error wrapping
- Use slog for any logging (none needed in storage currently)
- Follow Go naming conventions
  </action>
  <verify>
Run `make dev` and check ./tmp/air-combined.log compiles without errors. The storage package should be importable.
  </verify>
  <done>internal/storage/storage.go exists and compiles. Exports: New, PathForUUID, DirForUUID, CopyAndHash, HashFile, EnsureDirectories, FileExists, Delete, BasePath.</done>
</task>

</tasks>

<verification>
1. Migration applied: `docker exec -it docko-db-1 psql -U docko -c "\dt"` shows all 7 new tables
2. sqlc generated: `ls internal/database/sqlc/*.sql.go` shows documents.sql.go and jobs.sql.go
3. Storage package compiles: `make dev` runs without errors in ./tmp/air-combined.log
4. Schema correctness: `docker exec -it docko-db-1 psql -U docko -c "\d documents"` shows all columns
</verification>

<success_criteria>
- Migration 003_documents.sql creates: documents, jobs, document_events, tags, correspondents, document_tags, document_correspondents tables
- Jobs table has index for SKIP LOCKED dequeue performance
- sqlc queries generate without errors
- Storage service handles UUID path sharding and hash-while-copy pattern
- All code follows existing codebase conventions (error wrapping, no fmt.Printf)
</success_criteria>

<output>
After completion, create `.planning/phases/01-foundation/01-01-SUMMARY.md`
</output>
